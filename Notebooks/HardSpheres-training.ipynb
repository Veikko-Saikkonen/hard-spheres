{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "from IPython.display import display\n",
    "\n",
    "# For OS-agnostic paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "from copy import deepcopy\n",
    "import glob, json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import mlflow\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from src.utils import load_raw_data\n",
    "from src.plotting import plot_pointcloud, plot_sample_figures\n",
    "from src.models.HardSphereGAN import GAN\n",
    "from src.models.StaticScaler import StaticMinMaxScaler\n",
    "from src.HSDataset import HSDataset\n",
    "\n",
    "%cd -\n",
    "\n",
    "plt.set_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard spheres model development\n",
    "\n",
    "\n",
    "First stage: Develop a CNN - based GAN to work with ordered point clouds.'\n",
    "\n",
    "This notebook is an attempt at the simpler hexagonal and square lattices after slow progress in the full-scale experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phis = [0.84] # Add more phis here\n",
    "\n",
    "path = Path(\"../data/raw/samples\")\n",
    "path = Path(\"../data/raw/crystal/Hex\")\n",
    "path = Path(\"../data/raw/crystal/Sq\")\n",
    "\n",
    "\n",
    "files, dataframe, metadata = load_raw_data(path, phi=phis, subpath=\"disorder-0.2\")\n",
    "# files, dataframe, metadata = load_raw_data(path, phi=phis)\n",
    "\n",
    "dataframe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hex lattice\n",
    "\n",
    "N = 1600 \n",
    "X_box = 41.076212368516387\n",
    "Y_box = 35.573043402379753   \n",
    " \n",
    "# Square lattice\n",
    "N = 1600\n",
    "X_box = 38.225722823651111\n",
    "y_box = 38.225722823651111 \n",
    "dataframe[\"r\"] = 0.375 # Fixed radius for all data, for square lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StaticMinMaxScaler(\n",
    "    columns = [\"x\", \"y\", \"r\"],\n",
    "    maximum = [X_box, Y_box, 2*X_box], # NOTE: Tuned for physical feasibility\n",
    "    minimum = [-X_box, -Y_box, 0] # NOTE: Tuned for physical feasibility\n",
    ")\n",
    "\n",
    "dataframe_scaled = pd.DataFrame(scaler.transform(dataframe), columns=dataframe.columns)\n",
    "\n",
    "dataframe_scaled.set_index(dataframe.index, inplace=True)\n",
    "\n",
    "dataframe_scaled = dataframe_scaled.drop(columns=[\"class\"]) # Redundant with r\n",
    "# dataframe_scaled = dataframe_scaled.sort_values(by=[\"experiment\", \"sample\"])\n",
    "dataframe_scaled.describe().round(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataframe_scaled.copy().query(\"(sample=='sample-1')\").loc[:,[\"x\", \"y\", \"r\"]].reset_index(drop=True)\n",
    "sample = torch.tensor(sample.values).unsqueeze(0)\n",
    "print(sample.shape)\n",
    "plot_pointcloud(sample[0], plot_radius=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {\n",
    "    \"training\":{\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\", # MPS is not supported by PyTorch 2D TransposeConv\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 5000,\n",
    "        \"early_stopping_patience\": 50,\n",
    "        \"early_stopping_headstart\": 0,\n",
    "        \"early_stopping_tolerance\": 1e-3, # Gradient norm based\n",
    "        \"log_image_frequency\": 5,\n",
    "        \"generator_headstart\": 0,\n",
    "        \"training_ratio_dg\": 3,\n",
    "        \"optimizer_g\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.00005, # 0.00005, #0.002, \n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": (0.5, 0.999)\n",
    "        },\n",
    "        \"optimizer_d\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.00005, #0.002, \n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": (0.5, 0.999)\n",
    "        },\n",
    "        \"d_loss\":{\n",
    "            \"name\": \"CryinGANDiscriminatorLoss\", # CryinGANDiscriminatorLoss for WaGAN + L1 loss, BCELoss for baseline\n",
    "            \"mu\": 1.0, # L1 loss coefficient\n",
    "        },\n",
    "        \"g_loss\":{\n",
    "            \"name\": \"HSGeneratorLoss\",\n",
    "            \"radius_loss\": 0,\n",
    "            \"grid_density_loss\": 0,\n",
    "            \"gan_loss\": 1,\n",
    "            \"distance_loss\": 0,\n",
    "            \"physical_feasibility_loss\": 0,\n",
    "            \"coefficients\":{\n",
    "                \"gan_loss\": 1,\n",
    "                \"radius_loss\": 0,\n",
    "                \"grid_density_loss\": 0,\n",
    "                \"physical_feasibility_loss\": 0,\n",
    "                \"distance_loss\": 0,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"dataset\":{\n",
    "        \"descriptor_list\": [\"phi\"],\n",
    "        \"synthetic_samples\":{\n",
    "            \"rotational\": 0,\n",
    "            \"shuffling\": 0,\n",
    "            \"spatial_offset_static\": 0.1\n",
    "            }, # NOTE: Could do subsquares and more rotations.\n",
    "        # \"downsample\": 0 #TODO: Remove\n",
    "        \"downsample\": 0.05 #TODO: Remove\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = HSDataset(\n",
    "    dataframe_scaled.copy(), # Dont use the ordering\n",
    "    **run_params[\"dataset\"]\n",
    "    )\n",
    "dataset.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "Create a GAN architecture, which creates point clouds $\\hat{y}$ based on the descriptor(s) $\\hat{X}$ and a random noise vector $\\hat{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.CryinGAN import CCCGDiscriminator, CCCGeneratorWithDiffusion, CCCGenerator\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "\n",
    "\n",
    "test_frac = 0.2\n",
    "\n",
    "dataset = dataset.to(run_params[\"training\"][\"device\"])\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [1-test_frac, test_frac])\n",
    "print(len(trainset), len(testset))\n",
    "\n",
    "out_samples = dataset.samples[0].shape[1]\n",
    "out_dimensions = 2 #dataset.samples[0].shape[2]\n",
    "\n",
    "kernel_size = (1,1)\n",
    "stride=1\n",
    "\n",
    "generator = CCCGenerator(\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    channels_coefficient=1,\n",
    "    rand_features=513,# 513 for one paper, 64 for another,\n",
    "    out_dimensions=out_dimensions,\n",
    "    out_samples=out_samples,\n",
    "    latent_dim=256, # 128 for the papers\n",
    "    fix_r=0.0049,\n",
    "    clip_output = False\n",
    "    # (\n",
    "    #     dataset.y.min(dim=0).values.min(dim=0).values,\n",
    "    #     dataset.y.max(dim=0).values.max(dim=0).values\n",
    "    # )\n",
    "    ).to(\"mps\")\n",
    "\n",
    "\n",
    "input_channels = 3\n",
    "\n",
    "discriminator = CCCGDiscriminator(\n",
    "    input_channels=input_channels, \n",
    "    in_samples=out_samples, \n",
    "    kernel_size=(1,1),\n",
    "    channels_coefficient=3\n",
    "    ).to(\"mps\")\n",
    "\n",
    "gan = GAN(\n",
    "    dataset, \n",
    "    dataset,# No separate test set\n",
    "    generator_model=generator,\n",
    "    discriminator_model=discriminator,\n",
    "    **run_params\n",
    "    )\n",
    "\n",
    "print(summary(gan.generator, input_data=sample_x, depth=2))\n",
    "print(summary(gan.discriminator, input_data=sample_y_mps, depth=2))\n",
    "\n",
    "_out = gan.generate(sample_x)[0]\n",
    "# _out = _out.numpy()\n",
    "\n",
    "plot_pointcloud(_out, plot_radius=False)\n",
    "10_603_201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.train_n_epochs(\n",
    "    epochs=run_params[\"training\"][\"epochs\"],\n",
    "    batch_size=run_params[\"training\"][\"batch_size\"],\n",
    "    experiment_name=\"Square lattice, small sample\",\n",
    ")\n",
    "\n",
    "generated = gan.generate(sample_x).cpu()\n",
    "\n",
    "plt.scatter(generated[0][:,0], generated[0][:,1].cpu())\n",
    "plt.title(\"Generated sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the discriminator with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the discriminator with random data\n",
    "\n",
    "# Generate random data\n",
    "random_data = torch.rand_like(sample_y).to(\"mps\")\n",
    "random_data = torch.randn_like(sample_y).to(\"mps\")\n",
    "print(random_data.shape)\n",
    "\n",
    "plot_pointcloud(random_data[0].cpu().numpy(), plot_radius=False)\n",
    "\n",
    "# Test the discriminator\n",
    "\n",
    "output = gan.discriminator(random_data)\n",
    "print(output.shape)\n",
    "print(\"Mean of discriminator output:\", output.mean().item())\n",
    "plt.title(f\"Discriminator output: {output[0].item()}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
