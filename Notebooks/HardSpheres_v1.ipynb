{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard spheres model development\n",
    "\n",
    "\n",
    "First stage: Develop a CNN - based GAN to work with ordered point clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "from IPython.display import display\n",
    "\n",
    "# For OS-agnostic paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "from copy import deepcopy\n",
    "import glob, json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import mlflow\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from src.utils import load_raw_data\n",
    "from src.plotting import plot_pointcloud, plot_sample_figures\n",
    "from src.models.HardSphereGAN import GAN\n",
    "from src.models.StaticScaler import StaticMinMaxScaler\n",
    "\n",
    "%cd -\n",
    "\n",
    "plt.set_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = [0.86] # Add more phis here\n",
    "\n",
    "path = Path(\"../data/raw/samples\")\n",
    "\n",
    "files, dataframe, metadata = load_raw_data(path, phi=phis)\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler = StaticMinMaxScaler(\n",
    "    columns = [\"x\", \"y\", \"r\"],\n",
    "    maximum = [22, 22, 22], # NOTE: Tuned for physical feasibility\n",
    "    minimum = [-22, -22, -22] # NOTE: Tuned for physical feasibility\n",
    "    # maximum = [21.74652425, 21.74652425, 1.62], # NOTE: Normal minmax\n",
    "    # minimum = [-21.74652425, -21.74652425, 0.73]  # NOTE: Normal minmax\n",
    "    # NOTE: Scale r with the higher x,y min,max -> Ensures everything is between 0,1 but also retains physical setup with linear scaling\n",
    ")\n",
    "\n",
    "dataframe_scaled = pd.DataFrame(scaler.transform(dataframe), columns=dataframe.columns)\n",
    "\n",
    "dataframe_scaled.set_index(dataframe.index, inplace=True)\n",
    "\n",
    "dataframe_scaled = dataframe_scaled.drop(columns=[\"class\"]) # Redundant with r\n",
    "# dataframe_scaled = dataframe_scaled.sort_values(by=[\"experiment\", \"sample\"])\n",
    "dataframe_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt: Order dataframe based on Xy coordinates\n",
    "\n",
    "This is done to introduce the spatial relationship that a CNN can utilize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = dataframe_scaled.copy()\n",
    "\n",
    "\n",
    "_X[\"Xy\"] = np.sqrt(((_X[\"x\"]-0.5)**2) + ((_X[\"y\"]-0.5)**2))\n",
    "_X[\"Xy\"] = _X[\"r\"]# + _X[\"y\"] # Think about starting from center and going radially outwards\n",
    "# _X[\"Xy\"] = _X[\"r\"] # Think about starting from center and going radially outwards\n",
    "\n",
    "_X = _X.query(\"experiment=='phi-0.86'&sample=='sample-1'\")\n",
    "\n",
    "_X = _X.sort_values(by=[\"r\", \"x\", \"y\"])\n",
    "\n",
    "_X = _X.reset_index(drop=True)\n",
    "\n",
    "_X\n",
    "\n",
    "plt.scatter(x=_X.values[:,0], y=_X.values[:,1], c=_X.index)\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than not ordered, but can be problematic as values in the middle have a high probability of ending up with non-neighbouring samples.\n",
    "\n",
    "However it is also a good experiment as if we observe that the corners behave better than the middle, we know we are on to something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_scaled_ordered = dataframe_scaled.copy()\n",
    "dataframe_scaled_ordered = dataframe_scaled_ordered.sort_values(by=[\"experiment\", \"sample\", \"r\"])\n",
    "dataframe_scaled_ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.HSDataset import HSDataset\n",
    "\n",
    "dataset = HSDataset(\n",
    "    dataframe_scaled_ordered.copy(), # Dont use the ordering\n",
    "    descriptor_list=[\"phi\", \"r\"],\n",
    "    synthetic_samples={\n",
    "        \"rotational\": 0,\n",
    "        \"shuffling\": 0,\n",
    "        \"spatial_offset_static\": 0,\n",
    "        \"spatial_offset_repeats\": 0\n",
    "        }, \n",
    "    downsample=0\n",
    "    )\n",
    "print(dataset[:][0].shape)\n",
    "print(dataset[:][1].shape)\n",
    "# Create a function that visualizes the point cloud\n",
    "\n",
    "plot_pointcloud(dataset[2][1], plot_radius=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointcloud(dataset[0][1], plot_radius=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointcloud(dataset[:][1][:,:,:].mean(dim=0), plot_radius=True)\n",
    "\n",
    "plt.title(\"Mean Point Cloud, n={}\".format(len(dataset[:][1][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Samples are order invariant in the sample size dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointcloud(reversed(dataset[2][1]), plot_radius=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "Create a GAN architecture, which creates point clouds $\\hat{y}$ based on the descriptor(s) $\\hat{X}$ and a random noise vector $\\hat{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a generator model as described in the paper\n",
    "# paper: https://arxiv.org/pdf/2404.06734\n",
    "\n",
    "in_features = 64\n",
    "kernel_size = (3,3) # if 3x3, the output x,y,r will correlate with each other\n",
    "stride = (1,1)\n",
    "\n",
    "from src.models.CryinGAN import Generator, CCCGenerator\n",
    "\n",
    "out_samples = dataset.samples[0].shape[1]\n",
    "\n",
    "out_dimensions = 3 \n",
    "\n",
    "generator_model_2 = CCCGenerator(\n",
    "    kernel_size=1,\n",
    "    stride=1,\n",
    "    rand_features=out_samples,\n",
    "    out_dimensions=out_dimensions,\n",
    "    latent_dim=30,\n",
    "    out_samples=out_samples,\n",
    "    fix_r=sample_y_mps[0,:,2]\n",
    "    ).to(\"mps\")\n",
    "\n",
    "# sample_x = sample_x.to(\"mps\")\n",
    "print(sample_x.shape)\n",
    "_out = generator_model_2(sample_x_mps).detach()\n",
    "print(_out.shape)\n",
    "\n",
    "# softmaxed_r = generator_model_2._softmax_radius_dimensions(_out)\n",
    "_out_torch = _out\n",
    "_out = _out.cpu().numpy()\n",
    "\n",
    "print(summary(generator_model_2, input_data=sample_x, depth=2))\n",
    "\n",
    "\n",
    "plot_pointcloud(_out[0,:,:3], plot_radius=True)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.models.CryinGAN import Discriminator2D, CCCGDiscriminator\n",
    "\n",
    "# Initialize the discriminator\n",
    "input_channels = sample_y.shape[-1] # For fractional coordinates\n",
    "in_samples = sample_y.shape[1] # For fractional coordinates\n",
    "discriminator_model_2 = CCCGDiscriminator(input_channels=input_channels, in_samples=in_samples).to(\"mps\")\n",
    "\n",
    "# Print the discriminator architecture\n",
    "\n",
    "# Example input with batch size of 16 and 3 input channels (for fractional coordinates)\n",
    "batch_size = 32\n",
    "# Generate output\n",
    "\n",
    "_sample_y = sample_y.to(\"mps\")\n",
    "\n",
    "print(_sample_y.shape)\n",
    "output = discriminator_model_2(_sample_y)\n",
    "print(output.shape)\n",
    "\n",
    "summary(discriminator_model_2, input_data=_sample_y, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.isclose(dataframe_scaled_ordered[\"r\"], 0.534689)).mean()\n",
    "dataframe_scaled_ordered[\"r\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {\n",
    "    \"comment\": \"Fixed radius to a real sample\",\n",
    "    \"training\":{\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\", # MPS is not supported by PyTorch 2D TransposeConv\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 5000,\n",
    "        \"early_stopping_patience\": 20,\n",
    "        \"early_stopping_headstart\": 0,\n",
    "        \"early_stopping_tolerance\": 1e-3, # Gradient norm based\n",
    "        \"log_image_frequency\": 3,\n",
    "        \"generator_headstart\": 0,\n",
    "        \"training_ratio_dg\": 3,\n",
    "        \"optimizer_g\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.001, # 0.00005, #0.002,  # 0.001\n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": (0.5, 0.999)\n",
    "        },\n",
    "        \"optimizer_d\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.001, #0.002, \n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": (0.5, 0.999)\n",
    "        },\n",
    "        \"d_loss\":{\n",
    "            \"name\": \"CryinGANDiscriminatorLoss\", # CryinGANDiscriminatorLoss for WaGAN + L1 loss, BCELoss for baseline\n",
    "            \"mu\": 0.5, # L1 loss coefficient\n",
    "        },\n",
    "        \"g_loss\":{\n",
    "            \"name\": \"HSGeneratorLoss\",\n",
    "            \"radius_loss\": 0,\n",
    "            \"grid_density_loss\": 0,\n",
    "            \"gan_loss\": 1,\n",
    "            \"distance_loss\": 0,\n",
    "            \"physical_feasibility_loss\": 0,\n",
    "            \"coefficients\":{\n",
    "                \"gan_loss\": 10,\n",
    "                \"radius_loss\": 0,\n",
    "                \"grid_density_loss\": 1,\n",
    "                \"physical_feasibility_loss\": 10,\n",
    "                \"distance_loss\": 10,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"dataset\":{\n",
    "        \"descriptor_list\": [\"phi\"],\n",
    "        \"synthetic_samples\":{\n",
    "            \"rotational\": 0,\n",
    "            \"shuffling\": 0,\n",
    "            \"spatial_offset_static\": 0.05,\n",
    "            \"spatial_offset_repeats\": 2\n",
    "            }, # NOTE: Could do subsquares and more rotations.\n",
    "        \"downsample\": 0,\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "dataframe_scaled_ordered_filtered = dataframe_scaled_ordered[np.isclose(dataframe_scaled_ordered[\"r\"], 0.521914)]\n",
    "\n",
    "dataset = HSDataset(\n",
    "    dataframe_scaled_ordered_filtered.copy(), # NOTE: Ordered -> ordered by radius.\n",
    "    **run_params[\"dataset\"]\n",
    "    )\n",
    "print(dataset.y.shape)\n",
    "\n",
    "plot_pointcloud(dataset[:][1][4], plot_radius=True)\n",
    "\n",
    "plt.title(\"Point Cloud, n={}\".format(len(dataset[:][1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.CryinGAN import CCCGDiscriminator, CCCGeneratorWithDiffusion, CCCGenerator\n",
    "\n",
    "test_frac = 0.2\n",
    "kernel_size = (3,3)\n",
    "dataset = dataset.to(run_params[\"training\"][\"device\"])\n",
    "\n",
    "dataset = dataset.to(run_params[\"training\"][\"device\"])\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [1-test_frac, test_frac])\n",
    "print(len(trainset), len(testset))\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "out_samples = dataset.samples[0].shape[1]\n",
    "out_dimensions = dataset.samples[0].shape[2]\n",
    "\n",
    "kernel_size = (1,1)\n",
    "stride=1\n",
    "\n",
    "\n",
    "generator = CCCGenerator(\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    channels_coefficient=1,\n",
    "    rand_features=513,# 513 for one paper, 64 for another,\n",
    "    out_dimensions=out_dimensions,\n",
    "    out_samples=out_samples,\n",
    "    latent_dim=128, # 128 for the papers\n",
    "    fix_r=sample_y_mps[0,:,2],\n",
    "    clip_output = False\n",
    "    # (\n",
    "    #     dataset.y.min(dim=0).values.min(dim=0).values,\n",
    "    #     dataset.y.max(dim=0).values.max(dim=0).values\n",
    "    # )\n",
    "    ).to(\"mps\")\n",
    "\n",
    "input_channels = 3\n",
    "\n",
    "discriminator = CCCGDiscriminator(\n",
    "    input_channels=input_channels, \n",
    "    in_samples=out_samples, \n",
    "    kernel_size=(1,1),\n",
    "    channels_coefficient=1\n",
    "    ).to(\"mps\")\n",
    "\n",
    "gan = GAN(\n",
    "    dataset, \n",
    "    dataset,# No separate test set\n",
    "    generator_model=generator,\n",
    "    discriminator_model=discriminator,\n",
    "    **run_params\n",
    "    )\n",
    "\n",
    "print(summary(gan.generator, input_data=sample_x_mps, depth=2))\n",
    "print(summary(gan.discriminator, input_data=sample_y_mps, depth=2))\n",
    "\n",
    "_out = gan.generate(sample_x)[0]\n",
    "\n",
    "plot_pointcloud(_out, plot_radius=True)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "10_603_201\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = out_samples\n",
    "\n",
    "# run 'mlflow server --host 127.0.0.1 --port 8080' before starting training\n",
    "\n",
    "gan.train_n_epochs(\n",
    "    epochs=run_params[\"training\"][\"epochs\"],\n",
    "    batch_size=run_params[\"training\"][\"batch_size\"],\n",
    "    experiment_name=f\"Fullscale, subsampled r, sample size = {sample_size}\",\n",
    "    requirements_file = Path(\"../top-level-requirements.txt\"),\n",
    "    save_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradients for plotting\n",
    "\n",
    "real_images = sample_y_mps\n",
    "fake_images = gan.generate(sample_x_mps).to(real_images.device)\n",
    "\n",
    "alpha = torch.rand(real_images.size(0), 1, 1).to(real_images.device)\n",
    "interpolates_coord = alpha * real_images + (1 - alpha) * fake_images\n",
    "interpolates_coord = interpolates_coord.requires_grad_(True)\n",
    "d_interpolates_coord = discriminator(interpolates_coord)\n",
    "\n",
    "grad_outputs_coord = torch.ones_like(d_interpolates_coord)\n",
    "\n",
    "gradients = torch.autograd.grad(\n",
    "    outputs=d_interpolates_coord,\n",
    "    inputs=interpolates_coord,\n",
    "    grad_outputs=grad_outputs_coord,\n",
    "    create_graph=True,\n",
    "    retain_graph=True,\n",
    "    only_inputs=True,\n",
    "    is_grads_batched=False,\n",
    ")[0]\n",
    "\n",
    "# Plot the points and the related gradients\n",
    "plt.figure(figsize=(10,10))\n",
    "# plt.scatter(x=real_images[0,:,0].cpu(), y=real_images[0,:,1].cpu(), c=\"blue\", alpha=0.5,marker=\".\", label=\"Real\")\n",
    "plt.scatter(x=interpolates_coord[0,:,0].cpu().detach(), y=interpolates_coord[0,:,1].cpu().detach(), c=\"green\", alpha=0.5,marker=\".\", label=\"Interpolated\")\n",
    "# plt.scatter(x=fake_images[0,:,0].cpu(), y=fake_images[0,:,1].cpu(), c=\"red\", alpha=0.5,marker=\".\",label=\"Fake\")\n",
    "\n",
    "plt.quiver(\n",
    "    interpolates_coord[0,:,0].cpu().detach(),\n",
    "    interpolates_coord[0,:,1].cpu().detach(),\n",
    "    gradients[0,:,0].cpu().detach(),\n",
    "    gradients[0,:,1].cpu().detach(),\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.xlim(.25,.75)\n",
    "plt.ylim(.25,.75)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = gan.generate(sample_x)[0]\n",
    "# _out = _out.numpy()\n",
    "\n",
    "plot_pointcloud(_out, plot_radius=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the discriminator with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the discriminator with random data\n",
    "\n",
    "# Generate random data\n",
    "random_data = torch.rand_like(sample_y).to(\"mps\")\n",
    "random_data = torch.randn_like(sample_y).to(\"mps\")\n",
    "print(random_data.shape)\n",
    "\n",
    "plot_pointcloud(random_data[0].cpu().numpy(), plot_radius=False)\n",
    "\n",
    "# Test the discriminator\n",
    "\n",
    "output = gan.discriminator(random_data)\n",
    "print(output.shape)\n",
    "print(\"Mean of discriminator output:\", output.mean().item())\n",
    "plt.title(f\"Discriminator output: {output[0].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile model performance for computational bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        gan.generator(sample_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        gan.discriminator(sample_y_mps)\n",
    "\n",
    "    # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN QUESTIONS\n",
    "\n",
    "- What to do with the metadata?\n",
    "- Class and radius are redundant. Is the real physical measure numerical or categorical / quantified? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.round(5).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet.dataset import ShapeNetDataset, ModelNetDataset\n",
    "from pointnet.model import PointNetCls, feature_transform_regularizer, PointNetfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(PointNetfeat(), input_size=(32, 3, 1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
