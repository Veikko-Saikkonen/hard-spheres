{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "from IPython.display import display\n",
    "\n",
    "# For OS-agnostic paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "from copy import deepcopy\n",
    "import glob, json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import mlflow\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from src.utils import load_raw_data\n",
    "from src.plotting import plot_pointcloud, plot_sample_figures\n",
    "from src.models.HardSphereGAN import GAN\n",
    "from src.models.StaticScaler import StaticMinMaxScaler\n",
    "\n",
    "%cd -\n",
    "\n",
    "plt.set_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard spheres model development\n",
    "\n",
    "\n",
    "First stage: Develop a CNN - based GAN to work with ordered point clouds.'\n",
    "\n",
    "This notebook is an attempt at the simpler hexagonal and square lattices after slow progress in the full-scale experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phis = [0.84] # Add more phis here\n",
    "\n",
    "path = Path(\"../data/raw/crystal/Sq\")\n",
    "path = Path(\"../data/raw/crystal/Hex\")\n",
    "\n",
    "\n",
    "files, dataframe, metadata = load_raw_data(path, phi=phis, subpath=\"disorder-0.2\")\n",
    "# files, dataframe, metadata = load_raw_data(path, phi=phis)\n",
    "\n",
    "dataframe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hex lattice\n",
    "\n",
    "N = 1600 \n",
    "X_box = 41.076212368516387\n",
    "Y_box = 35.573043402379753   \n",
    " \n",
    "# # Square lattice\n",
    "# N = 1600\n",
    "# X_box = 38.225722823651111\n",
    "# y_box = 38.225722823651111 \n",
    "# dataframe[\"r\"] = 0.375 # Fixed radius for all data, for square lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StaticMinMaxScaler(\n",
    "    columns = [\"x\", \"y\", \"r\"],\n",
    "    maximum = [X_box, Y_box, 2*X_box], # NOTE: Tuned for physical feasibility\n",
    "    minimum = [-X_box, -Y_box, 0] # NOTE: Tuned for physical feasibility\n",
    ")\n",
    "\n",
    "dataframe_scaled = pd.DataFrame(scaler.transform(dataframe), columns=dataframe.columns)\n",
    "\n",
    "dataframe_scaled.set_index(dataframe.index, inplace=True)\n",
    "\n",
    "dataframe_scaled = dataframe_scaled.drop(columns=[\"class\"]) # Redundant with r\n",
    "dataframe_scaled = dataframe_scaled.sort_values(by=[\"experiment\", \"sample\"])\n",
    "dataframe_scaled.describe().round(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataframe_scaled.copy().query(\"(sample=='sample-1')\").loc[:,[\"x\", \"y\", \"r\"]].reset_index(drop=True)\n",
    "sample = torch.tensor(sample.values).unsqueeze(0)\n",
    "print(sample.shape)\n",
    "plot_pointcloud(sample[0], plot_radius=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_scaled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build dataset\n",
    "from src.HSDataset import HSDataset\n",
    "\n",
    "dataset = HSDataset(\n",
    "    dataframe_scaled.copy(), # Dont use the ordering\n",
    "    descriptor_list=[\"phi\"],\n",
    "    synthetic_samples={\"rotational\": 0, \"shuffling\": 0, \"spatial_offset_static\": 0, \"spatial_offset_repeats\": 0}, \n",
    "    downsample=264 / 1600, # 264 is the number of particles in the paper\n",
    "    keep_r=False\n",
    "    )\n",
    "print(dataset[:][0].shape)\n",
    "print(dataset[:][1].shape)\n",
    "\n",
    "# Create a function that visualizes the point cloud\n",
    "plot_pointcloud(dataset[0][1], plot_radius=False)\n",
    "plot_pointcloud(dataset[-1][1], plot_radius=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointcloud(dataset[:][1][:,:,:].mean(dim=0), plot_radius=False)\n",
    "\n",
    "plt.title(\"Mean Point Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "Create a GAN architecture, which creates point clouds $\\hat{y}$ based on the descriptor(s) $\\hat{X}$ and a random noise vector $\\hat{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a generator model as described in the paper\n",
    "# paper: https://arxiv.org/pdf/2404.06734\n",
    "\n",
    "in_features = 64\n",
    "kernel_size = (3,3) # if 3x3, the output x,y,r will correlate with each other\n",
    "stride = (1,1)\n",
    "\n",
    "from src.models.CryinGAN import Generator, CCCGenerator\n",
    "\n",
    "out_samples = dataset.samples[0].shape[1]\n",
    "\n",
    "generator_model_2 = CCCGenerator(\n",
    "    # kernel_size=1, stride=1,\n",
    "    # rand_features=64, \n",
    "    # out_dimensions=2, \n",
    "    # fix_r=0.5, \n",
    "    # out_samples=out_samples\n",
    "    clip_output= False,\n",
    "    fix_r=0.0049,\n",
    "    kernel_size=[1,1],\n",
    "    latent_dim=128,\n",
    "    out_dimensions=2,\n",
    "    out_samples=264,\n",
    "    rand_features=64,\n",
    "    channels_coefficient=1,\n",
    "    stride=1\n",
    "    ).to(\"mps\")\n",
    "\n",
    "print(sample_x.shape)\n",
    "_out = generator_model_2(sample_x).detach().cpu()\n",
    "print(_out.shape)\n",
    "\n",
    "print(summary(generator_model_2, input_data=sample_x, depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a discriminator model as described in the paper\n",
    "from src.models.CryinGAN import CCCGDiscriminator\n",
    "\n",
    "\n",
    "discriminator_model = CCCGDiscriminator(\n",
    "  in_samples=264,\n",
    "  input_channels=2,\n",
    "  kernel_size=[1,1],\n",
    "  channels_coefficient=1,\n",
    "  latent_dim=1056,\n",
    "  ).to(\"mps\")\n",
    "\n",
    "print(summary(discriminator_model, input_data=sample_y_mps, depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample_y[0][:,0].cpu(), sample_y[0][:,1].cpu(), c=list(range(len(sample_y[0][:,0]))), cmap=\"viridis\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_out_samples = N # Max size\n",
    "downsample = 264 / 1600\n",
    "fix_r = 0.0049\n",
    "\n",
    "if downsample:\n",
    "    out_samples = int(max_out_samples * downsample)\n",
    "else:\n",
    "    out_samples = max_out_samples\n",
    "kernel_size = [1,1]\n",
    "stride = 1\n",
    "\n",
    "run_params = {\n",
    "    \"comment\": \"CCCGenerator\",\n",
    "    \"training\":{\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\", # MPS is not supported by PyTorch 2D TransposeConv\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 5000,\n",
    "        \"early_stopping_patience\": 20,\n",
    "        \"early_stopping_headstart\": 0,\n",
    "        \"early_stopping_tolerance\": 1e-3, # Gradient norm based\n",
    "        \"log_image_frequency\": 3,\n",
    "        \"generator_headstart\": 0,\n",
    "        \"training_ratio_dg\": 3,\n",
    "        \"optimizer_g\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.001, # 0.00005, #0.002, \n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": [0.5, 0.999]\n",
    "        },\n",
    "        \"optimizer_d\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\": 0.001, #0.002, \n",
    "            # \"hypergrad_lr\": 1e-6,\n",
    "            \"weight_decay\": 0,\n",
    "            \"betas\": [0.5, 0.999]\n",
    "        },\n",
    "        \"d_loss\":{\n",
    "            \"name\": \"CryinGANDiscriminatorLoss\", # CryinGANDiscriminatorLoss for WaGAN + L1 loss, BCELoss for baseline\n",
    "            \"mu\": 1.0, # L1 loss coefficient\n",
    "        },\n",
    "        \"g_loss\":{\n",
    "            \"name\": \"HSGeneratorLoss\",\n",
    "            \"radius_loss\": 0,\n",
    "            \"grid_density_loss\": 1,\n",
    "            \"gan_loss\": 1,\n",
    "            \"distance_loss\": 1,\n",
    "            \"physical_feasibility_loss\": 0,\n",
    "            \"grid_order_loss\": 1,\n",
    "            \"coefficients\":{\n",
    "                \"grid_order_k\": 4,\n",
    "                \"grid_order_loss\": 1,\n",
    "                \"gan_loss\": 1,\n",
    "                \"radius_loss\": 0,\n",
    "                \"grid_density_loss\": 100,\n",
    "                \"physical_feasibility_loss\": 0,\n",
    "                \"distance_loss\": 100,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"dataset\":{\n",
    "        \"descriptor_list\": [\"phi\"],\n",
    "        \"synthetic_samples\":{\n",
    "            \"rotational\": 1,\n",
    "            \"shuffling\": 0,\n",
    "            \"spatial_offset_static\": 0.05,\n",
    "            \"spatial_offset_repeats\": 2\n",
    "            }, # NOTE: Could do subsquares and more rotations.\n",
    "        \"downsample\": downsample,\n",
    "        \"keep_r\": False\n",
    "    },\n",
    "    \"generator\": {\n",
    "        \"class\": \"CCCGenerator\",\n",
    "        \"kernel_size\": kernel_size,\n",
    "        \"stride\": stride,\n",
    "        \"channels_coefficient\": 1,\n",
    "        \"rand_features\": 64,# 513 for one paper, 64 for another,\n",
    "        \"out_dimensions\": 2,\n",
    "        \"out_samples\": out_samples,\n",
    "        \"latent_dim\": 128, # 128 for the papers\n",
    "        \"fix_r\": fix_r,\n",
    "        \"clip_output\": False\n",
    "    },\n",
    "    \"discriminator\": {\n",
    "        \"class\": \"CCCGDiscriminator\",\n",
    "        \"input_channels\": 2, \n",
    "        \"in_samples\": out_samples, \n",
    "        \"kernel_size\": [1,1],\n",
    "        \"channels_coefficient\": 3\n",
    "    },\n",
    "    \"metrics\":{\n",
    "        \"packing_fraction\": True,\n",
    "        \"packing_fraction_fix_r\": fix_r,\n",
    "        \"packing_fraction_box_size\": 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = HSDataset(\n",
    "    dataframe_scaled.copy(), # Dont use the ordering\n",
    "    **run_params[\"dataset\"]\n",
    "    )\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(dataset.y.shape)\n",
    "plt.scatter(sample_y[0][:,0].cpu(), sample_y[0][:,1].cpu(), c=list(range(len(sample_y[0]))), cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "test_frac = 0.2\n",
    "\n",
    "dataset = dataset.to(run_params[\"training\"][\"device\"])\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [1-test_frac, test_frac])\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "\n",
    "gan = GAN(\n",
    "    dataset, \n",
    "    dataset,# No separate test set\n",
    "    **run_params\n",
    "    )\n",
    "\n",
    "print(summary(gan.generator, input_data=sample_x_mps, depth=2))\n",
    "print(summary(gan.discriminator, input_data=sample_y_mps, depth=2))\n",
    "\n",
    "_out = gan.generate(sample_x)[0]\n",
    "\n",
    "plot_pointcloud(_out, plot_radius=False)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "10_603_201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "run_params_yaml = Path(\"../experiments/1-baseline.yaml\")\n",
    "\n",
    "with open(run_params_yaml, \"w\") as f:\n",
    "    yaml.dump(run_params, f, )\n",
    "\n",
    "# Read the parameters from the yaml to make sure it works\n",
    "with open(run_params_yaml, \"r\") as f:\n",
    "    run_params_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# Make sure the parameters are the same\n",
    "run_params == run_params_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HSDataset(\n",
    "    dataframe_scaled.copy(), # Dont use the ordering\n",
    "    **run_params_yaml[\"dataset\"]\n",
    "    )\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(dataset.y.shape)\n",
    "plt.scatter(sample_y[0][:,0].cpu(), sample_y[0][:,1].cpu(), c=list(range(len(sample_y[0]))), cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "test_frac = 0.2\n",
    "\n",
    "dataset = dataset.to(run_params_yaml[\"training\"][\"device\"])\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [1-test_frac, test_frac])\n",
    "\n",
    "sample_x = dataset[0:32][0].cpu()#.transpose(-1,-2)\n",
    "sample_y = dataset[0:32][1].cpu()\n",
    "\n",
    "sample_x_mps = sample_x.to(\"mps\")\n",
    "sample_y_mps = sample_y.to(\"mps\")\n",
    "\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "\n",
    "gan = GAN(\n",
    "    dataset, \n",
    "    dataset,# No separate test set\n",
    "    **run_params_yaml\n",
    "    )\n",
    "\n",
    "print(summary(gan.generator, input_data=sample_x_mps, depth=2))\n",
    "print(summary(gan.discriminator, input_data=sample_y_mps, depth=2))\n",
    "\n",
    "_out = gan.generate(sample_x)[0]\n",
    "\n",
    "plot_pointcloud(_out, plot_radius=False)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0,1)\n",
    "10_603_201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = out_samples\n",
    "\n",
    "gan.train_n_epochs(\n",
    "    epochs=run_params_yaml[\"training\"][\"epochs\"],\n",
    "    batch_size=run_params_yaml[\"training\"][\"batch_size\"],\n",
    "    experiment_name=f\"Hex lattice, sample size = {sample_size}\",\n",
    "    requirements_file = Path(\"../top-level-requirements.txt\"),\n",
    "    save_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/94662023d37747e89a6e769bd9d8aa63/discriminator'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "data = (dataframe_scaled.query(\"sample=='sample-1'\").loc[:,[\"x\", \"y\", \"r\"]].values[::20].reshape(1, 80, 3))\n",
    "data = data.astype(np.float32)\n",
    "print(data.shape)\n",
    "\n",
    "loaded_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Alternative generator with diffusion\n",
    "# generator = CCCGeneratorWithDiffusion(\n",
    "#     kernel_size=kernel_size,\n",
    "#     channels_coefficient=1,\n",
    "#     stride=stride,\n",
    "#     rand_features=513,\n",
    "#     out_dimensions=out_dimensions,\n",
    "#     out_samples=out_samples,\n",
    "#     latent_dim=256, # initial latent channels\n",
    "#     fix_r=0.0049,\n",
    "#     clip_output = False\n",
    "#     # (\n",
    "#     #     dataset.y.min(dim=0).values.min(dim=0).values,\n",
    "#     #     dataset.y.max(dim=0).values.max(dim=0).values\n",
    "#     #     )\n",
    "#     ).to(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the discriminator with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the discriminator with random data\n",
    "\n",
    "# Generate random data\n",
    "random_data = torch.rand_like(sample_y).to(\"mps\")\n",
    "random_data = torch.randn_like(sample_y).to(\"mps\")\n",
    "print(random_data.shape)\n",
    "\n",
    "plot_pointcloud(random_data[0].cpu().numpy(), plot_radius=False)\n",
    "\n",
    "# Test the discriminator\n",
    "\n",
    "output = gan.discriminator(random_data)\n",
    "print(output.shape)\n",
    "print(\"Mean of discriminator output:\", output.mean().item())\n",
    "plt.title(f\"Discriminator output: {output[0].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the weights on the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights from the first layer of the generator\n",
    "first_layer_weights = gan.generator.model[0].weight.data.cpu().numpy()\n",
    "\n",
    "max_filters = 64\n",
    "first_layer_weights = first_layer_weights[:, :max_filters]\n",
    "\n",
    "# Plot the weights\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(1,first_layer_weights.shape[1]):\n",
    "    plt.subplot(8, 8, i)\n",
    "    plt.imshow(first_layer_weights[i].reshape(8, 8), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    # Add a global colorbar\n",
    "    if i == 1:\n",
    "        plt.colorbar()\n",
    "        # Relocate the colorbar\n",
    "        plt.gcf().axes[-1].set_position([0.95, 0.1, 0.03, 0.8])\n",
    "\n",
    "    plt.title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle('Weights of the First Layer of the Generator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights from the first layer of the generator\n",
    "first_layer_weights = gan.discriminator.fc_layers[-3].weight.data.cpu().numpy()\n",
    "\n",
    "# Plot the weights\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(1,first_layer_weights.shape[0]):\n",
    "    plt.subplot(5, 2, i)\n",
    "    plt.imshow(first_layer_weights[:,i].reshape(5, 2), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    # Add a global colorbar\n",
    "    if i == 1:\n",
    "        plt.colorbar()\n",
    "        # Relocate the colorbar\n",
    "        plt.gcf().axes[-1].set_position([0.95, 0.1, 0.03, 0.8])\n",
    "\n",
    "    plt.title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle('Weights of the First Layer of the Generator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
