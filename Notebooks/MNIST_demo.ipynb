{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard spheres model demo on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "from IPython.display import display\n",
    "\n",
    "# For OS-agnostic paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "from copy import deepcopy\n",
    "import glob, json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def load_mnist():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32))\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform, target_transform=target_transform)\n",
    "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform, target_transform=target_transform)\n",
    "    \n",
    "    return trainset, testset\n",
    "\n",
    "trainset, testset = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock data\n",
    "\n",
    "Make the digit (label) the input based on which the gan generates data.\n",
    "\n",
    "The goal is to train the gan to generate images of the digit that is fed as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the digit (label) the input based on which the gan generates data.\n",
    "\n",
    "# The goal is to train the gan to generate images of the digit that is fed as the input.\n",
    "\n",
    "class MNISTLabelDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return label, image\n",
    "    \n",
    "trainset = MNISTLabelDataset(trainset)\n",
    "testset = MNISTLabelDataset(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.),\n",
       " tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588,\n",
       "           -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,\n",
       "            1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,\n",
       "            0.9843,  0.9843,  0.9843,  0.9843,  0.7647,  0.3490,  0.9843,\n",
       "            0.8980,  0.5294, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.6157,  0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n",
       "            0.9843,  0.9843,  0.9843,  0.9686, -0.2706, -0.3569, -0.3569,\n",
       "           -0.5608, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.8588,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n",
       "            0.5529,  0.4275,  0.9373,  0.8902, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078,\n",
       "           -0.9137, -1.0000, -0.6627,  0.2078, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902,\n",
       "           -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843,\n",
       "           -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,\n",
       "            0.7647,  0.2549, -0.1529, -0.9922, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,\n",
       "            0.8824,  0.9843,  0.9843, -0.0667, -0.8039, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.6471,  0.4588,  0.9843,  0.9843,  0.1765, -0.7882, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8745, -0.2706,  0.9765,  0.9843,  0.4667, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000,  0.9529,  0.9843,  0.9529, -0.4980,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.6392,  0.0196,  0.4353,  0.9843,  0.9843,  0.6235, -0.9843,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,\n",
       "            0.7961,  0.9843,  0.9843,  0.9843,  0.9608,  0.4275, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,\n",
       "            0.9843,  0.9843,  0.9843,  0.5765, -0.3882, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,\n",
       "            0.9843,  0.5529, -0.3647, -0.9843, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n",
       "            0.3412,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294,\n",
       "           -0.3725, -0.9294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,\n",
       "            0.9843,  0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,\n",
       "            0.9843,  0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]))"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc3f6917e50>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample\n",
    "\n",
    "label, image = trainset[0]\n",
    "print(label)\n",
    "plt.imshow(image.squeeze().numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.float32\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for digit, real_images in DataLoader(trainset, batch_size=64, shuffle=True):\n",
    "    print(digit.shape)\n",
    "    print(digit.dtype)\n",
    "    print(real_images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_layer = nn.ConvTranspose2d(24, 1, kernel_size=16, stride=1, padding=1) # 1x1 -> 3x3\n",
    "_layer2 = nn.ConvTranspose2d(1, 1, kernel_size=17, stride=1, padding=1) # 3x3 -> 9x9 # Adds 4 pixels on each side\n",
    "\n",
    "# The layers add kernel_size - 2 - stride + 1\n",
    "\n",
    "_inp = torch.randn(64, 24, 1, 1)\n",
    "\n",
    "predicted_shape = (\n",
    "    _inp.shape[-1]\n",
    "    + _layer.kernel_size[0]\n",
    "    - 2\n",
    "    - _layer.stride[0]\n",
    "    + _layer2.kernel_size[0]\n",
    "    - 2\n",
    "    - _layer2.stride[0]\n",
    ")\n",
    "print(predicted_shape)\n",
    "_layer2(_layer(_inp)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img=3, features_d=12):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=3\n",
    "            ),  # 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d, 3),  # 16x16\n",
    "            self._block(features_d, features_d * 2, 3,),  # 8x8\n",
    "            self._block(features_d * 2, features_d, 3),  # 4x4\n",
    "            nn.Conv2d(features_d, 1, kernel_size=5),  # 1x1\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.out = nn.Sequential( # This is the output layer that will take the output of the discriminator and the digit and make a decision whether the image is real or fake\n",
    "            nn.Linear(50,32), # 441 + 1 for digit\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32,32), # 441 + 1 for digit\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32,1), # 441 + 1 for digit\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, digit):\n",
    "        disc_out = self.disc(x)\n",
    "        digit = (digit-5)/10 # Normalize the digit to be between -0.4 and 0.4\n",
    "        linear = torch.cat((disc_out, digit.unsqueeze(1)), dim=1)\n",
    "        return self.out(linear)\n",
    "\n",
    "\n",
    "class MNISTGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=100, channels_img=1, features_g=64, output_dim=28, n_blocks=2):\n",
    "        super(MNISTGenerator, self).__init__()\n",
    "        # Input: (batch, z_dim, 1, 1)\n",
    "        # Output: (batch, channels_img, output_dim, output_dim)\n",
    "\n",
    "        remainder = (output_dim-1) % n_blocks # 28 % 3 = 1\n",
    "        add_per_block = (output_dim-1) // n_blocks # 5 kernel size adds 3 \n",
    "        kernel_size = 3+add_per_block # 3 blocks from 1 -> 28, need to add 28 / 3 pixels per block\n",
    "        final_kernel_size = kernel_size + remainder # needs to be larger to cover the remainder\n",
    "        blocks = [] # E.g. from 1 to 9 in 3 steps, 1 -> 4 (kernel size 7) -> 7 -> 9\n",
    "        for i in range(n_blocks):\n",
    "            if i == 0:\n",
    "                blocks.append(self._block(z_dim, features_g, kernel_size, 1, 1))\n",
    "            elif i < n_blocks-1:\n",
    "                blocks.append(self._block(features_g, features_g, kernel_size, 1, 1))\n",
    "            else:\n",
    "                blocks.append(self._block(features_g, channels_img, final_kernel_size, 1, 1))\n",
    "\n",
    "        blocks.append(nn.Tanh())\n",
    "        self.gen = nn.Sequential(*blocks)\n",
    "\n",
    "        self.z_model = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.digit_injector = nn.Sequential(\n",
    "            nn.Linear(1, z_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, digit):\n",
    "        # Create an input vector for the generator, sized correctly (based on z_dim)\n",
    "        batch_size = digit.shape[0]\n",
    "\n",
    "        # Create a random vector of size z_dim\n",
    "        x = torch.randn(batch_size, self.z_dim)\n",
    "\n",
    "        # Pass the random vector through the z_model\n",
    "        x = self.z_model(x)\n",
    "        # Add the digit to the random vector after passing it through the digit_injector\n",
    "        inj_digit = self.digit_injector(digit.unsqueeze(1))\n",
    "        x = (x + inj_digit) # Enabled for now\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        fake_image = self.gen(x)\n",
    "        return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MNISTGenerator                           [2, 1, 28, 28]            --\n",
      "├─Sequential: 1-1                        [2, 32]                   --\n",
      "│    └─Linear: 2-1                       [2, 32]                   1,056\n",
      "│    └─LeakyReLU: 2-2                    [2, 32]                   --\n",
      "│    └─Linear: 2-3                       [2, 32]                   1,056\n",
      "│    └─LeakyReLU: 2-4                    [2, 32]                   --\n",
      "├─Sequential: 1-2                        [2, 32]                   --\n",
      "│    └─Linear: 2-5                       [2, 32]                   64\n",
      "│    └─LeakyReLU: 2-6                    [2, 32]                   --\n",
      "│    └─Linear: 2-7                       [2, 32]                   1,056\n",
      "│    └─LeakyReLU: 2-8                    [2, 32]                   --\n",
      "├─Sequential: 1-3                        [2, 1, 28, 28]            --\n",
      "│    └─Sequential: 2-9                   [2, 8, 7, 7]              --\n",
      "│    │    └─ConvTranspose2d: 3-1         [2, 8, 7, 7]              20,736\n",
      "│    │    └─BatchNorm2d: 3-2             [2, 8, 7, 7]              16\n",
      "│    │    └─ReLU: 3-3                    [2, 8, 7, 7]              --\n",
      "│    └─Sequential: 2-10                  [2, 8, 13, 13]            --\n",
      "│    │    └─ConvTranspose2d: 3-4         [2, 8, 13, 13]            5,184\n",
      "│    │    └─BatchNorm2d: 3-5             [2, 8, 13, 13]            16\n",
      "│    │    └─ReLU: 3-6                    [2, 8, 13, 13]            --\n",
      "│    └─Sequential: 2-11                  [2, 8, 19, 19]            --\n",
      "│    │    └─ConvTranspose2d: 3-7         [2, 8, 19, 19]            5,184\n",
      "│    │    └─BatchNorm2d: 3-8             [2, 8, 19, 19]            16\n",
      "│    │    └─ReLU: 3-9                    [2, 8, 19, 19]            --\n",
      "│    └─Sequential: 2-12                  [2, 1, 28, 28]            --\n",
      "│    │    └─ConvTranspose2d: 3-10        [2, 1, 28, 28]            1,152\n",
      "│    │    └─BatchNorm2d: 3-11            [2, 1, 28, 28]            2\n",
      "│    │    └─ReLU: 3-12                   [2, 1, 28, 28]            --\n",
      "│    └─Tanh: 2-13                        [2, 1, 28, 28]            --\n",
      "==========================================================================================\n",
      "Total params: 35,538\n",
      "Trainable params: 35,538\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 9.34\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.18\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 0.32\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Discriminator                            [64, 1]                   --\n",
       "├─Sequential: 1-1                        [64, 49]                  --\n",
       "│    └─Conv2d: 2-1                       [64, 4, 26, 26]           40\n",
       "│    └─LeakyReLU: 2-2                    [64, 4, 26, 26]           --\n",
       "│    └─Sequential: 2-3                   [64, 4, 26, 26]           --\n",
       "│    │    └─Conv2d: 3-1                  [64, 4, 26, 26]           144\n",
       "│    │    └─BatchNorm2d: 3-2             [64, 4, 26, 26]           8\n",
       "│    │    └─LeakyReLU: 3-3               [64, 4, 26, 26]           --\n",
       "│    └─Sequential: 2-4                   [64, 8, 26, 26]           --\n",
       "│    │    └─Conv2d: 3-4                  [64, 8, 26, 26]           288\n",
       "│    │    └─BatchNorm2d: 3-5             [64, 8, 26, 26]           16\n",
       "│    │    └─LeakyReLU: 3-6               [64, 8, 26, 26]           --\n",
       "│    └─Sequential: 2-5                   [64, 4, 26, 26]           --\n",
       "│    │    └─Conv2d: 3-7                  [64, 4, 26, 26]           288\n",
       "│    │    └─BatchNorm2d: 3-8             [64, 4, 26, 26]           8\n",
       "│    │    └─LeakyReLU: 3-9               [64, 4, 26, 26]           --\n",
       "│    └─Conv2d: 2-6                       [64, 1, 22, 22]           101\n",
       "│    └─MaxPool2d: 2-7                    [64, 1, 7, 7]             --\n",
       "│    └─Flatten: 2-8                      [64, 49]                  --\n",
       "│    └─Sigmoid: 2-9                      [64, 49]                  --\n",
       "├─Sequential: 1-2                        [64, 1]                   --\n",
       "│    └─Linear: 2-10                      [64, 32]                  1,632\n",
       "│    └─LeakyReLU: 2-11                   [64, 32]                  --\n",
       "│    └─Linear: 2-12                      [64, 32]                  1,056\n",
       "│    └─LeakyReLU: 2-13                   [64, 32]                  --\n",
       "│    └─Linear: 2-14                      [64, 1]                   33\n",
       "│    └─Sigmoid: 2-15                     [64, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 3,614\n",
       "Trainable params: 3,614\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 36.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 12.74\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 12.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = MNISTGenerator(z_dim=32, channels_img=1, features_g=8, output_dim=28, n_blocks=4)\n",
    "discriminator = Discriminator(channels_img=1, features_d=4)\n",
    "\n",
    "print(summary(generator, input_size=(2,), device='cpu', depth=3))\n",
    "summary(discriminator,input_size=((64,1,28,28), (64,)))\n",
    "# summary(gan, input_size=((64,), (64,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GAN class that takes in the generator and discriminator, and the training data\n",
    "# The GAN class will be responsible for training the generator and discriminator\n",
    "# The GAN should train the generator to generate images of the digit that is fed as the input\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator, trainset, testset, device='cpu', batch_size=32):\n",
    "        super(GAN, self).__init__()\n",
    "        self.trainset = trainset\n",
    "        self.testset = testset\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.generator = generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.002)\n",
    "        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n",
    "\n",
    "    def train_n_epochs(self, epochs, batch_size=32):\n",
    "        print(f'Starting...\\nTime: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        for epoch in range(epochs):\n",
    "            self._train_epoch(epoch, batch_size=batch_size)\n",
    "\n",
    "    def forward(self, digit):\n",
    "        return self.generate(digit)\n",
    "\n",
    "    def _train_epoch(self, epoch, batch_size=32):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        mean_loss_d = 0\n",
    "        mean_loss_g = 0\n",
    "\n",
    "\n",
    "        for digits, real_images in DataLoader(self.trainset, batch_size=batch_size, shuffle=True):\n",
    "   \n",
    "            real_images = real_images.to(self.device)\n",
    "            digits = digits.to(self.device)\n",
    "\n",
    "            real_labels = torch.ones(real_images.size(0), 1).to(self.device)\n",
    "            fake_labels = torch.zeros(real_images.size(0), 1).to(self.device)\n",
    "\n",
    "            # Train the discriminator\n",
    "            self.d_optimizer.zero_grad()\n",
    "\n",
    "            real_outputs = self.discriminator(real_images, digits.to(self.device))\n",
    "            # print(real_outputs.shape, real_labels.shape)\n",
    "            d_loss_real = self.criterion(real_outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "\n",
    "            fake_images = self.generator(digits)\n",
    "            fake_outputs = self.discriminator(fake_images, digits)\n",
    "            d_loss_fake = self.criterion(fake_outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "            # Train the generator\n",
    "            self.g_optimizer.zero_grad()\n",
    "\n",
    "            fake_images = self.generator(digits)\n",
    "            fake_outputs = self.discriminator(fake_images, digits)\n",
    "            g_loss = self.criterion(fake_outputs, real_labels) # We want the generator to generate images that the discriminator thinks are real\n",
    "            g_loss.backward()\n",
    "\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            mean_loss_d += d_loss.item()\n",
    "            mean_loss_g += g_loss.item()\n",
    "\n",
    "\n",
    "        print(f'Time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")},Epoch {epoch}, D loss: {d_loss.item()}, G loss: {g_loss.item()}')\n",
    "\n",
    "        mean_loss_d /= len(self.trainset)\n",
    "        mean_loss_g /= len(self.trainset)\n",
    "\n",
    "        return mean_loss_d, mean_loss_g\n",
    "\n",
    "    def generate(self, digit):\n",
    "        self.generator.eval()\n",
    "        return self.generator(digit.to(self.device)).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(generator, discriminator, trainset, testset, device='cpu')\n",
    "# Train the GAN\n",
    "# gan.train_n_epochs(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Time: 2024-05-10 18:18:06\n",
      "Time: 2024-05-10 18:19:31,Epoch 0, D loss: 100.0, G loss: 0.0\n",
      "Time: 2024-05-10 18:20:57,Epoch 1, D loss: 100.0, G loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[442], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_n_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[440], line 22\u001b[0m, in \u001b[0;36mGAN.train_n_epochs\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[440], line 49\u001b[0m, in \u001b[0;36mGAN._train_epoch\u001b[0;34m(self, epoch, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# print(real_outputs.shape, real_labels.shape)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(real_outputs, real_labels)\n\u001b[0;32m---> 49\u001b[0m \u001b[43md_loss_real\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(digits)\n\u001b[1;32m     52\u001b[0m fake_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(fake_images, digits)\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train_n_epochs(10, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2024-05-10 17:55:33,Epoch 0, D loss: 1.9694897446242976e-07, G loss: 15.510581970214844\n",
      "Time: 2024-05-10 17:57:20,Epoch 1, D loss: 4.4035989077428894e-08, G loss: 17.131580352783203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_n_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[386], line 21\u001b[0m, in \u001b[0;36mGAN.train_n_epochs\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_n_epochs\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[386], line 50\u001b[0m, in \u001b[0;36mGAN._train_epoch\u001b[0;34m(self, epoch, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m d_loss_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     49\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(digits)\n\u001b[0;32m---> 50\u001b[0m fake_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(fake_outputs, fake_labels)\n\u001b[1;32m     52\u001b[0m d_loss_fake\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[384], line 37\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x, digit)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, digit):\n\u001b[0;32m---> 37\u001b[0m     disc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     digit \u001b[38;5;241m=\u001b[39m (digit\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# Normalize the digit to be between -0.4 and 0.4\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     linear \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((disc_out, digit\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hard-spheres/.venv/lib/python3.9/site-packages/torch/nn/functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "\n",
    "gan.train_n_epochs(10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2024-05-04 23:15:55,Epoch 0, D loss: 1.2664172649383545, G loss: 0.7814494371414185\n",
      "Time: 2024-05-04 23:18:12,Epoch 2, D loss: 1.6521220207214355, G loss: 0.9710193276405334\n",
      "Time: 2024-05-04 23:20:30,Epoch 4, D loss: 0.8520565629005432, G loss: 1.512847900390625\n",
      "Time: 2024-05-04 23:22:46,Epoch 6, D loss: 1.3171820640563965, G loss: 0.9108135104179382\n",
      "Time: 2024-05-04 23:25:03,Epoch 8, D loss: 0.9192835688591003, G loss: 1.4069147109985352\n"
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "\n",
    "gan.train_n_epochs(10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9-5)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc3f90c43a0>"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demo the GAN\n",
    "\n",
    "digit = torch.tensor([5.0])\n",
    "\n",
    "fake_images = gan(digit)\n",
    "\n",
    "plt.imshow(fake_images[0].squeeze().detach().numpy(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
